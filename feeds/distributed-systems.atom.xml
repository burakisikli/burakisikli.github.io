<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title></title><link href="http://www.burakisikli.com/" rel="alternate"></link><link href="http://www.burakisikli.com/feeds/distributed-systems.atom.xml" rel="self"></link><id>http://www.burakisikli.com/</id><updated>2009-06-11T16:23:00+03:00</updated><entry><title>Dağıtık Sistemler(Distributed Systems)-1</title><link href="http://www.burakisikli.com/dagitik-sistemlerdistributed-systems-1.html" rel="alternate"></link><published>2009-06-11T16:23:00+03:00</published><author><name>burakisikli</name></author><id>tag:www.burakisikli.com,2009-06-11:dagitik-sistemlerdistributed-systems-1.html</id><summary type="html">&lt;p&gt;Dağıtık sistemlerde en çok kullanılan program Hadoop'tur. Hadoop, Java
programlama dilinde yazılmış framework’tür. Amacı Google Dosya Sistemi
teknolojisi olan Map-Reduce algoritmasını birçok bilgisayara dağıtarak
uygulayı sağlamaktır.&lt;/p&gt;
&lt;p&gt;Google, Yahoo ve Facebook gibi şirketler devasa boyutlarda veriyi analiz
etmek için artık SQL, RDBMS kullanmıyorlar. Hadoop adlı  anahtar-değer
tekniğini kullanan bir ürünü kullanıyorlar. Arama motoru pazarında
başarılı olmak öncelikle &lt;em&gt;teknolojik altyapının dağıtımlı çalışması&lt;/em&gt;ndan
geçiyor. Yani bütün arama, depolama, ve indeksleme gibi işleri birden
fazla bilgisayara dağıtarak yapmak. Hatta böyle bir altyapısı olduğu
için Google’dan &lt;em&gt;aslında bir dağıtımlı bilgisayar şirketi&lt;/em&gt; diye
bahsedilir. Yani herkes internetten bilgileri toplayıp depolayabilir ama
bunu yüksek performansta yapmak yükü bir bilgisayar tarlasına
dağıtabilmekten geçiyor. Bilgisayarlar arasındaki bu iş bölümünü Google
kendi geliştirdiği MapReduce denilen bir yazılım platformu ile yapıyor.
Ancak MapReduce ile aynı işi yapan ve açık kaynaklı olan bir yazılım
platformu daha var: Hadoop.&lt;/p&gt;
&lt;p&gt;Hadoop'un temelindeki teknik "eşle/indirge (map/reduce)" adindaki bir
tekniktir. Bu teknik Apache tarafindan open source ortama getirildi ve
Hadoop bu şekilde dünyaya geldi. Teknolojinin çıkış noktası Google; bu
şirketin Web'den çektiği terabaytlarca veriyi analiz etmesi gerektiği
için eşle/indirge teknolojisini geliştirdiler, geliştirmeye mecbur
oldular.&lt;/p&gt;
&lt;p&gt;Eşle/indirge nedir? Online tabanlarda anahtar değeri belli bir makinaya
yönlendirmek için kullanılıyor. Eşle/indirge tabanlarında ise, anahtar
değeri eşle sürecinde her node(düğüm) üzerindeki veriye bakılarak bir
"özet" çıkarmak için kullanılıyor. Indirge sürecinde ise her ayrı
node'daki anahtar-deger çiftleri alınarak uyuşan anahtarlar bir daha
birleştiriliyor. Bu son birleşim bize nihai sonucu veriyor.&lt;/p&gt;
&lt;p&gt;Hadoop, yatay olarak ölçeklenebilen bir teknoloji, bu sebeple kümedeki
her makina kendi verisine sahip, yani tüm işlem o makinaya yerel. Aynı
online dünyada oldugu gibi (mesela Voldemort) her node kendi verisini
çöküşten kurtulmak amacıyla kümede birkaç diger bilgisayara yedek olarak
gönderebilmekte. Hadoop'ta değişik olan bir "idare edici" yani "görev
verici" makinanın olması. Burada bir mahsur yok; Eşle/indirge arka
planda çalışan bir teknoloji olduğu için dağıtıcı kondüktör sistem, eger
iyi tasarlanmışsa, bir hassas nokta haline gelmez. İşin en ağır kısmı
zaten kümedeki işlemci nodelar tarafindan üstlenmekte. Hadoop'ta
yapılacak işlemler tipik olarak bir "Job" olarak yazılıyor. Job'ınızı
yazıyorsunuz, ve Hadoop kümenize veriyorsunuz.&lt;/p&gt;
&lt;p&gt;Bir örnek düşünelim: Elimizde koca bir dosya var, bu dosyada her satırda
bir meyve ismi, ve o meyve satın alındığında ne kadar para ödenmiş
olduğu yazılı olsun (alttaki koca bir dosya değil ama olduğunu
düşünelim). Amacımız her meyve için toplam ne kadar harcandığını
bulmak.&lt;br /&gt;
armut 10&lt;br /&gt;
portakal 3&lt;br /&gt;
incir 9&lt;br /&gt;
armut 9&lt;br /&gt;
armut 10&lt;br /&gt;
incir 3&lt;br /&gt;
mandalin 2&lt;br /&gt;
erik 29&lt;/p&gt;
&lt;p&gt;Burada anahtar değerleri meyve isimleri olacak. Diyelim ki Hadoop
kümemizde 2 node var, ve üstteki dosyayı Hadoop'a verdik.&lt;/p&gt;
&lt;p&gt;Hadoop bu dosyayı iki eşit parcaya bölecektir (anahtar değerlerine hiç
bakmadan, burası online'dan farklı), ve diyelim ki bölünme tam ortadan
yapıldı:&lt;br /&gt;
armut 10&lt;br /&gt;
portakal 3&lt;br /&gt;
incir 9&lt;br /&gt;
armut 9  &lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;armut 10&lt;br /&gt;
incir 3&lt;br /&gt;
mandalin 2&lt;br /&gt;
incir 29&lt;/p&gt;
&lt;p&gt;Her node, kendi içinde "eşle" yaparken, benzer anahtar degerlerini aynı
toplama yazacak. Bölüm 1) armut = 19, portakal = 3, incir = 9. Bolum 2)
armut 10, incir = 32, mandalin = 2.&lt;/p&gt;
&lt;p&gt;Her bölümün işi böylece bitiyor, yani eşle sahfası sona eriyor. Bundan
sonra "indirge" kısmında her bölümdeki anahtarlar bir de kendi
aralarında toplanıyorlar. Böylece armut = 29, portakal = 3, incir = 41,
mandalin = 2. Bu en son sonuç.&lt;/p&gt;
&lt;p&gt;Aktarılan bilgilere göre, saşırtıcı derecede çok sayıdaki analiz işlemi,
üstteki eşle/indirge mentalitesine uyarlanabilmektedir.&lt;/p&gt;
&lt;p&gt;Haberdar olunması gereken müthiş örneklerden biri, makina ögrenimi
algoritmalarının aynı eşle/indirge sistemine uyarlanabilmiş olmasıdır.
Mahout projesi bunu open source ortamına taşımaya aday oldu. Rapor
edilen sonuçlara göre eklenen her mikroişlemci core'u (Hadoop node'u
gibi) eşle/indirge ile kodlanmış makina ögrenimi algoritmasını bir o
kadar hızlandırmaktadır. Yani performans kazanımı "lineer" şekilde
artabilmektedir. Her eklenen core bir o kadar hızlanma getirmektedir.&lt;/p&gt;
&lt;p&gt;Hadoop’un Google’un MapReduce’undan en büyük farkı tabii ki açık
kaynaklı olması. Dolayısıyla Hadoop kullanarak isteyen herkes Google
kadar hızlı çalışabilen bir arama motoru yapabilir, tabii bir miktar
bütçeyle bir bilgisayar tarlası kurabiliyorsanız. Burada durup bir kere
daha düşünün. Hadoop açık kaynaklı olduğundan Google’a bir değil
binlerce rakip çıkabilir.&lt;br /&gt;
&lt;em&gt;MapReduce işlemleri küçük parçalara bölüyor ve farklı bilgisayarlara
dağıtıyor, sonra işlenenleri toplayıp sonucu veriyor.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Hadoop giderek bir endüstri standardı olmaya başlıyor. Mesela Facebook
Hadoop kullanarak kullanıcı davranışlarının analizini yapıyor (50 milyon
kişi ve ilişkileri) ve sosyal reklamların etkisini ölçüyor. Geçtiğimiz
aylarda New York Times bilgi işlem ekibi Hadoop kullanarak 150 yıllık
arşivindeki 11 milyon makaleyi dijitalleştirdi ve aranabilir hale
getirdi. Normalde aylar sürebilecek bilgi işleme bir kaç günde
bitirildi. Amazon ile Hadoop kullanarak EC2 (dağııtmlı işlemci) ve S3
(dağıtımlı depolama) servislerinden faydalanabilirsiniz.&lt;/p&gt;
&lt;p&gt;Hadoop projesini başlatan Doug Cutting aynı zamanda Yahoo ArGe bölümünde
çalışmaya başladı, haliyle Yahoo içinde arama dahil bir çok başka bilgi
işleme sisteminin performansını geliştiriyor. Daha fazla geliştiricinin
katılmasıyla Hadoop giderek daha da iyileşiyor ve tabii üniversitelerde
de yayılmaya başlıyor. Hadoop kullanabilmek / programlayabilmek önemli
bir beceri haline geliyor. Sonuçta Hadoop kullanabilen yeni mezunlar
piyasaya çıktıkça sadece Google gibi şirketler değil daha fazla kişi
veya şirket yüksek performanslı iş yapabilecek.&lt;/p&gt;
&lt;p&gt;Bir zamanlar dağıtımlı bilgisayar sistemlerine &lt;em&gt;bilgisayar tarlası&lt;/em&gt;
denilmekteydi, bugünlerde ise &lt;em&gt;bilgisayar bulutu&lt;/em&gt; (“cloud computing”)
diyoruz. Çok daha dinamik bir dünyanın tasviri bu. Nasıl bugün herkesin
kişisel bilgisayarı varsa yakında hepimiz günlük hayatımızdaki bilgileri
düzenleyebilmek için bilgisayar bulutu kullanıyor olabiliriz. Aslında
yaptığımız her Google aramasında kullanıyoruz bile.&lt;/p&gt;</summary><category term="Dağıtık"></category><category term="Distributed"></category><category term="Hadoop"></category><category term="Sistemler"></category><category term="Systems"></category></entry></feed>